# Peguix AI â€” Offline AI Web Application

<a>
    <img src="frontend/src/components/assets/logo.png" width="75" />
</a>


Peguix AI is a **fully offline AI web application** built using **React (frontend)** and **Node.js (backend)**, powered locally by **Ollama** with **LLaMA 3.2 Vision**.  
It focuses on **privacy, speed, and local inference** without relying on cloud APIs.

---

## ğŸŒ Frontend Demo (UI Preview Only)

A **frontend-only demo** is available to showcase the UI and layout of **Peguix AI**:

ğŸ‘‰ **Demo Link:** https://peguix-ai.netlify.app/

---

## âš ï¸ Demo Limitations

- This demo shows **UI only**
- âŒ AI chat, voice, and vision **will NOT work**
- âŒ Backend and Ollama are **not connected**
- âŒ No local inference in the demo

---

## âœ… Purpose of Demo

- UI/UX preview
- Design reference
- Navigation & layout showcase

---

For **full functionality**, the project must be run **locally with Ollama installed**.


## ğŸš€ Features

- ğŸ’¬ Offline AI Chat (local inference)
- ğŸ–¼ï¸ Vision-based image understanding (LLaMA 3.2 Vision)
- ğŸ™ï¸ Voice input (Speech-to-Text)
- âš¡ Fast responses (runs on local machine)
- ğŸ”’ No data sent to cloud
- ğŸ¨ Modern ChatGPT-style UI
- ğŸ–¥ï¸ Desktop-focused (PC optimized)

---

## ğŸ§  Tech Stack

### Frontend
- React
- CSS (custom UI)
- Web Speech API (for mic input)

### Backend
- Node.js
- Express
- Multer (image upload)
- Ollama (local LLM engine)

### AI Models
- **LLaMA 3.2 Vision**
- Whisper.cpp (for voice input)

---

## âš ï¸ Important Note (Please Read)

This project is **NOT meant to run on live hosting** such as GitHub Pages, Vercel, or Netlify.

### Why?
- Requires **Ollama running locally**
- Uses **local AI models**
- Requires **system-level dependencies**
- Backend communicates with `localhost`

âœ… The project works **perfectly in offline / local setup**  
âŒ It will **not work on public live URLs**

This repository is shared for **learning, demonstration, and development reference**.

---

### ğŸ“… Project Information
Created: Jan 31, 2026

---

## ğŸ› ï¸ How to Run Locally

### 1ï¸âƒ£ Install Ollama
Download and install Ollama from:
https://ollama.com

### 2ï¸âƒ£ Pull the Vision Model
```bash
ollama pull llama3.2-vision
